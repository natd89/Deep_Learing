{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import data, io, filters, transform\n",
    "import random\n",
    "from pdb import set_trace as brake\n",
    "import math\n",
    "import PIL\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images = os.listdir('/home/nmd89/Pictures/celeb_images')\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "def fc(x, out_size=None, is_output=False, name=\"fc\"):\n",
    "\n",
    "    #x is an input tensor\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        #  Create a W filter variable with the proper size\n",
    "        W_fc = tf.get_variable(name+'_W', shape=[x_shape[1], out_size], dtype=tf.float32, initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "        #  Create a B bias variable with the proper size\n",
    "        B_fc = tf.get_variable(initializer = tf.random_normal([out_size]), name = name+'_B')\n",
    "\n",
    "        #  Multiply x by W and add b\n",
    "        out = tf.nn.bias_add(tf.matmul(x, W_fc), B_fc)\n",
    "\n",
    "        #  If is_output is False,\n",
    "        if not is_output:\n",
    "\n",
    "            # Call the tf.nn.relu function\n",
    "            out = tf.nn.relu(out, name = name+'relu_fc')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv( x, filter_size=3, stride=1, num_filters=64, is_output=False, name='conv', initializer=tf.contrib.layers.l2_regularizer(scale=0.1)):\n",
    "\n",
    "    # x is an input tensor\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    # Declare a name scope using the \"name\" parameter\n",
    "    with tf.variable_scope('weights_'+name):\n",
    "\n",
    "        # Create a W filter variable with the proper size\n",
    "        W_conv = tf.get_variable(name+'_W', shape = [filter_size,filter_size,x_shape[3],num_filters], dtype=tf.float32, initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        \n",
    "        # Create a B bias variable withthe proper size\n",
    "        B_conv = tf.Variable(tf.random_normal([num_filters]), name = name+'_B')\n",
    "\n",
    "        # Convolve x with W by calling the tf.nn.conv2d function\n",
    "        convolve = tf.nn.conv2d(x,W_conv, strides = [1, stride, stride, 1], padding='SAME')\n",
    "    with tf.name_scope(name):\n",
    "        # Add the bias\n",
    "        convolve_bias = tf.nn.bias_add(convolve,B_conv)\n",
    "        # If is_output is False call the tf.nn.relu function\n",
    "        # if not is_output:\n",
    "        convolve_bias = tf.nn.relu(convolve_bias, name=name+'relu_conv')\n",
    "        # convolve_bias = tf.maximum(convolve_bias, 0.2*convolve_bias)\n",
    "\n",
    "    return convolve_bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def up_conv(x, name='upconv', filter_size=3, num_filters=64, stride=2, is_output=False):\n",
    "\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    with tf.variable_scope('weights_'+name):\n",
    "\n",
    "        W_up = tf.get_variable(name+'_W_up', shape = [filter_size,filter_size,num_filters,x_shape[3]], dtype = tf.float32, initializer = tf.contrib.layers.variance_scaling_initializer()) \n",
    "\n",
    "        out_shape = [x_shape[0],x_shape[1]*stride,x_shape[2]*stride,num_filters]\n",
    "\n",
    "        if not is_output:\n",
    "            h = tf.contrib.layers.layer_norm(x, center=True)\n",
    "        else:\n",
    "            h = x\n",
    "\n",
    "        transpose = tf.nn.conv2d_transpose(h, W_up, out_shape, [1,stride,stride,1], name = name)\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        up_con = tf.nn.relu(transpose, name=name+'relu_upconv')\n",
    "\n",
    "        # if not is_output:\n",
    "            # up_con = tf.nn.relu(transpose, name=name+'relu_upconv')\n",
    "        # if is_output:\n",
    "            # up_con = tf.nn.tanh(transpose, name=name+'_tanh')\n",
    "\n",
    "    return transpose\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discriminator( x, reuse=False, name='discriminator'):\n",
    "\n",
    "    x_shape = tf.shape(x)\n",
    "    batch_size = x_shape[0]\n",
    "    \n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()    \n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        h0 = conv( x, filter_size=3, stride=2, num_filters=64, name='disc_1') \n",
    "        h1 = conv( h0, filter_size=3, stride=2, num_filters=128, name='disc_2') \n",
    "        h2 = conv( h1, filter_size=3, stride=2, num_filters=256, name='disc_3')\n",
    "        h3 = conv( h2, filter_size=3, stride=2, num_filters=512, name='disc_4')\n",
    "        h4 = conv( h3, filter_size=3, stride=2, num_filters=1024, name='disc_5')\n",
    "        h5 = tf.reshape( h4, [batch_size,4*4*1024])\n",
    "        h6 = fc( h5, out_size=1, is_output=True, name='disc_fc')\n",
    "    \n",
    "    return h6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generator( z, reuse=False, name='generator' ):\n",
    "  \n",
    "    z_shape = tf.shape(z)\n",
    "    batch_size = z_shape[0]\n",
    "\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        fc_out = fc(z, out_size=4*4*1024, name='gen_fc')\n",
    "        h0 = tf.reshape(fc_out, [batch_size, 4, 4, 1024], name = 'gen_reshape')\n",
    "        h1 = up_conv(h0, name='gen_1', num_filters=512)\n",
    "        h2 = up_conv(h1, name='gen_2', num_filters = 256)\n",
    "        h3 = up_conv(h2, name='gen_3', num_filters = 128)\n",
    "        # h4 = up_conv(h3, name='gen_4', num_filters = 3)\n",
    "        h4 = up_conv(h3, name='gen_4', num_filters = 64)\n",
    "        h5 = up_conv(h4, name='gen_5', num_filters = 3)\n",
    "\n",
    "    return h5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lamb=10\n",
    "n_critic=5\n",
    "alpha=0.0001\n",
    "beta_1=0.0\n",
    "beta_2 = 0.9\n",
    "im_size = 128\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32, shape=[batch_size, 100], name='z')\n",
    "image = tf.placeholder(tf.float32, shape=[batch_size, im_size, im_size, 3], name='image')\n",
    "eps = tf.placeholder(tf.float32, shape = [batch_size, 1, 1, 1], name='eps')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope('Model'):\n",
    "    \n",
    "    # get the generated image\n",
    "    \n",
    "    G = generator(z)\n",
    "    \n",
    "    # get discriminator value\n",
    "    dx = discriminator(image, name='input_discriminator')\n",
    "    \n",
    "    x_hat = eps*image + (1-eps)*G\n",
    "    \n",
    "    d_xtilde = discriminator(G, name='x_tilde_discriminator', reuse=True)\n",
    "    d_xhat   = discriminator(x_hat, name='x_hat_discriminator', reuse=True)\n",
    "    \n",
    "    with tf.name_scope('Loss'):\n",
    "        # compute the loss function\n",
    "        L = d_xtilde - dx + lamb * ( tf.norm( tf.gradients( d_xhat, x_hat ) , 2 ) -1 )**2\n",
    "\n",
    "    # get variables to train \n",
    "    vars = tf.trainable_variables()\n",
    "    disc_params = [v for v in vars if 'disc' in v.name]\n",
    "    gen_params  = [v for v in vars if 'gen' in v.name]\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "with tf.name_scope('Optimizers'):\n",
    "     \n",
    "    disc_weights = tf.train.AdamOptimizer(alpha, beta_1, beta_2).minimize(tf.reduce_mean(L), var_list=[disc_params])\n",
    "      \n",
    "    gen_weights  = tf.train.AdamOptimizer(alpha, beta_1, beta_2).minimize(tf.reduce_mean(-d_xtilde), var_list=[gen_params])\n",
    "    tf.add_to_collection('disc_weights', disc_weights)\n",
    "    tf.add_to_collection('gen_weights', gen_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "\n",
    "    # summaries\n",
    "    writer = tf.summary.FileWriter('./tf_logs', sess.graph)\n",
    "    gen_im = tf.summary.image('generated_image', G, max_outputs = batch_size)\n",
    "    input_im = tf.summary.image('input_image', image, max_outputs = batch_size)\n",
    "    merge = tf.summary.merge_all()\n",
    "    z_input = np.random.uniform(size=(batch_size, 100))\n",
    "    # run the training loop\n",
    "    for i in range(100000):\n",
    "        # z_input = np.random.uniform(size=(batch_size, 100))\n",
    "        for j in range(n_critic):\n",
    "            \n",
    "            rand_index = random.sample(range(200000), batch_size)\n",
    "            rand_index = [300]\n",
    "\n",
    "            # z_input = np.random.uniform(size=(batch_size, 100))\n",
    "\n",
    "            loaded_images = np.zeros((batch_size, 128, 128, 3))\n",
    "            resized_images = np.zeros((batch_size, im_size, im_size, 3))\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                # loaded_images[k,:,:,:] = io.imread('/home/nmd89/Pictures/celeb_images/'+images[rand_index[k]])\n",
    "                loaded_images[k,:,:,:] = io.imread('/home/nmd89/Pictures/celeb_images/'+images[rand_index[k]]) \n",
    "                # resized_images[k,:,:,:] = transform.resize(loaded_images[k,:,:,:], ( im_size, im_size, 3))\n",
    "\n",
    "            epsilon = np.random.uniform(size=(batch_size, 1, 1, 1))\n",
    "            \n",
    "            disc_opt, ss = sess.run([disc_weights, merge], feed_dict={z: z_input, image: loaded_images, eps: epsilon})\n",
    "            # disc_opt, ss = sess.run([disc_weights, merge], feed_dict={z: z_input, image: resized_images, eps: epsilon})\n",
    "\n",
    "        gen_opt, ss =sess.run([gen_weights, merge], feed_dict={z: z_input, image: loaded_images})\n",
    "        # gen_opt, ss =sess.run([gen_weights, merge], feed_dict={z: z_input, image: resized_images})\n",
    "        writer.add_summary(ss, i*n_critic + j)\n",
    "        saver.save(sess, \"GAN_data/GAN.ckpt\")\n",
    "        print 'iteration ' + str(i) + ' complete'\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
